{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/splAcharya/RNN_Text_Generation/blob/main/Generating_Text_Using_RNNS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        " \n",
        "The following tutortials were used to learn about text generation using RNNs. \n",
        "Majority of code was take from the tensorflow. The tutorial was simply redone to learn in depth.\n",
        "\n",
        "1.   Text Generation Using RNN: https://www.tensorflow.org/text/tutorials/text_generation\n",
        "2. https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "\n"
      ],
      "metadata": {
        "id": "_--8YtIRMeZA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecrd-hQu7yGO"
      },
      "source": [
        "## Generating Text Reccurent Neural Networks\n",
        "\n",
        "This notebook attmepts to generate Story for Alice in Wonderland, using Gated Recurent Units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZYCHAHj8fLf"
      },
      "source": [
        "## Import Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K6scRoLe8jPU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wlF08spp80vX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-PpaAg19Dd0"
      },
      "source": [
        "## Load Bad Jokes Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUkiEMHYAYSU",
        "outputId": "9b7c7fb5-0bdd-477c-9245-db8669a99ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  170k  100  170k    0     0   355k      0 --:--:-- --:--:-- --:--:--  354k\n"
          ]
        }
      ],
      "source": [
        "!curl https://www.gutenberg.org/files/11/11-0.txt -o alice.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azVKuoLP9E0N",
        "outputId": "7b99d991-b4b9-4013-dce8-e1dbfb56bc29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 167808 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(\"alice.txt\", 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cpNUq-15p55y"
      },
      "outputs": [],
      "source": [
        "#raw_text[:1485]\n",
        "#raw_text[148847 :]\n",
        "text = text[1485 : 148860]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW4o9KAqCEcB",
        "outputId": "20fde977-4f49-4702-8a04-6fe8aa8beeb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "147375"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXFJRO5LqTRX",
        "outputId": "4bd40dc9-306f-43f7-ebae-ff91853a04e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zUKV1gR3qjId"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYxoGktzqolu"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQbJGxmqZb5Y"
      },
      "source": [
        "### Vectorize the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqHT66wlqkEJ",
        "outputId": "e83cf9ff-d3bb-4485-f501-549b4c22eff1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "--J9zspCq59d"
      },
      "outputs": [],
      "source": [
        "#Now create the tf.keras.layers.StringLookup layer:\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbYU4AoQrbs6",
        "outputId": "b8be91c2-b460-49fc-ebc6-ddc7cd7f0b41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[43, 44, 45, 46, 47, 48, 49], [66, 67, 68]]>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AaT30C9Jrm89"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(),\n",
        "                                                                         invert=True,\n",
        "                                                                         mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIjP2dmfr9qP",
        "outputId": "39b4858a-0c3a-4d64-98b7-60dbba601ded"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeAfurCHsK_L",
        "outputId": "0ba32460-164d-4932-b2fe-31d40f4057d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JL3v1DGGw25w"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bOSaNVbExAsY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDZCCoJQsPqW",
        "outputId": "60dcbb9f-09de-4157-f69d-d34c0fe48808"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(147375,), dtype=int64, numpy=array([2, 1, 2, ..., 1, 2, 1])>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UQ4IbNfAvDgT"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBx5OZyBvMsr",
        "outputId": "e49ba7d1-43c7-4cbc-c727-fadc9a3f50c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "\r\n",
            "CHAPTER I.\r\n",
            "Down the Rabbit-Hole\r\n",
            "\r\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into\n",
            "the book her sister was reading, but it had no pictures or\n",
            "conversations in it, “and what is the use of a book,” thought Alice\n",
            "“without pictures or conversations?”\n",
            "\n",
            "So she was considering in her own mind (as well as she could, for the\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure of\n",
            "making a daisy-chain would be worth the trouble of getting up and\n",
            "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
            "close by her.\n",
            "\n",
            "There was nothing so _very_ remarkable in that; nor did Alice think it\n",
            "so _very_ much out of the way to hear the Rabbit say to itself, “Oh\n",
            "dear! Oh dear! I shall be late!” (when she thought it over afterwards,\n",
            "it occurred to her that she ought to have wondered at this, but at the\n",
            "time it all seemed quite natural); but when the Rabbit actually _t"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(1000):\n",
        "  print(chars_from_ids(ids).numpy().decode('UTF-8'), end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7UO6BUpvva2O"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTmrew6Xvp_v",
        "outputId": "534bf3be-7aa5-4682-a76c-008c517f5303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[ 2  1  2  1 16 21 14 29 33 18 31  3 22 10  2  1 17 57 65 56  3 62 50 47\n",
            "  3 31 43 44 44 51 62  9 21 57 54 47  2  1  2  1  2  1 14 54 51 45 47  3\n",
            " 65 43 61  3 44 47 49 51 56 56 51 56 49  3 62 57  3 49 47 62  3 64 47 60\n",
            " 67  3 62 51 60 47 46  3 57 48  3 61 51 62 62 51 56 49  3 44 67  3 50 47\n",
            " 60  3 61 51 61], shape=(101,), dtype=int64)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'\\r' b'\\n' b'\\r' b'\\n' b'C' b'H' b'A' b'P' b'T' b'E' b'R' b' ' b'I' b'.'\n",
            " b'\\r' b'\\n' b'D' b'o' b'w' b'n' b' ' b't' b'h' b'e' b' ' b'R' b'a' b'b'\n",
            " b'b' b'i' b't' b'-' b'H' b'o' b'l' b'e' b'\\r' b'\\n' b'\\r' b'\\n' b'\\r'\n",
            " b'\\n' b'A' b'l' b'i' b'c' b'e' b' ' b'w' b'a' b's' b' ' b'b' b'e' b'g'\n",
            " b'i' b'n' b'n' b'i' b'n' b'g' b' ' b't' b'o' b' ' b'g' b'e' b't' b' '\n",
            " b'v' b'e' b'r' b'y' b' ' b't' b'i' b'r' b'e' b'd' b' ' b'o' b'f' b' '\n",
            " b's' b'i' b't' b't' b'i' b'n' b'g' b' ' b'b' b'y' b' ' b'h' b'e' b'r'\n",
            " b' ' b's' b'i' b's'], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder= True)\n",
        "for seq in sequences.take(1):\n",
        "  print(seq)\n",
        "  print()\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJqvGE-nwbpF",
        "outputId": "aee9b0e4-e63c-4f6a-febc-f6f52558c951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'\\r\\n\\r\\nCHAPTER I.\\r\\nDown the Rabbit-Hole\\r\\n\\r\\n\\r\\nAlice was beginning to get very tired of sitting by her sis'\n",
            "b'ter on the\\r\\nbank, and of having nothing to do: once or twice she had peeped into\\r\\nthe book her sister'\n",
            "b' was reading, but it had no pictures or\\r\\nconversations in it, \\xe2\\x80\\x9cand what is the use of a book,\\xe2\\x80\\x9d though'\n",
            "b't Alice\\r\\n\\xe2\\x80\\x9cwithout pictures or conversations?\\xe2\\x80\\x9d\\r\\n\\r\\nSo she was considering in her own mind (as well as s'\n",
            "b'he could, for the\\r\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\r\\nmaking a d'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_3MVGyBy-MZ8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3EICOeA-Om2"
      },
      "source": [
        "<p> For model training purposes, for each each input we will need a label. The goal here is, for every character (input) set the next character as label </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "e1lo41U68UpB"
      },
      "outputs": [],
      "source": [
        "#for training need, (input and label), p\n",
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1] #skip last character\n",
        "  target_text = sequence[1 : ] #skip first character\n",
        "  return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77tTMzWtxFxx",
        "outputId": "f4f01eb5-cef8-478c-910e-4c84dd06134e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Tensorflo', 'ensorflow')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "split_input_target(\"Tensorflow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nRMWOgVr-40k"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMZgurrv-_aG",
        "outputId": "fe4d1893-5fbf-47ee-ad26-43eda4f47abc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: b'\\r\\n\\r\\nCHAPTER I.\\r\\nDown the Rabbit-Hole\\r\\n\\r\\n\\r\\nAlice was beginning to get very tired of sitting by her si'\n",
            "Target: b'\\n\\r\\nCHAPTER I.\\r\\nDown the Rabbit-Hole\\r\\n\\r\\n\\r\\nAlice was beginning to get very tired of sitting by her sis'\n"
          ]
        }
      ],
      "source": [
        "for in_eg, tar_eg in dataset.take(1):\n",
        "  print(f\"Input: {text_from_ids(in_eg)}\")\n",
        "  print(f\"Target: {text_from_ids(tar_eg)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yTKHpQBR_Z8I"
      },
      "outputs": [],
      "source": [
        "#create training set\n",
        "BATCH_SIZE = 64 #number of data per batch for trainning\n",
        "\n",
        "#Buffer size to shiffle dataset\n",
        "BUFFER_SIZE = 20000\n",
        "dataset= (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wDQXN2q-BAPP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GklkjcafBSOa"
      },
      "source": [
        "## Build RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vLa-y-4zBWo2"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jWzMAR8RnBgW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "b07K7OB6BW-P"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "s9F-WtyNEeeM"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units = rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GC7ogTgUbglr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "n-OtfOQHXMp3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2095Z-zpcAa"
      },
      "source": [
        "## Try the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMhXpsFGBXA2",
        "outputId": "51892b63-421c-4fae-c7aa-b03be6c21d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 75) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJDdsd4sBdL8",
        "outputId": "2265c078-8e0d-49e4-d982-25fd270b6628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  19200     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  76875     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,034,379\n",
            "Trainable params: 4,034,379\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "T4mBmkMnVBVe"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeGidr9HBdml"
      },
      "source": [
        "## Sample Output \n",
        "*Other wasys to sample output----*\n",
        "(https://medium.com/deep-learning-with-keras/sampling-in-text-generation-b2f4825e1dad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "uODbqk5SBXED"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW9tY1NmUFTJ",
        "outputId": "439e2b11-788f-438b-b706-d18c9f04b00a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24, 26,  1, 39, 42, 67, 48,  6, 33, 72, 35, 68, 57, 21, 27, 25,  0,\n",
              "       49,  0, 60, 53, 53, 55, 24, 73, 47, 38, 43, 46, 48, 30, 10, 15,  4,\n",
              "       30,  2, 49,  1, 62, 18, 45,  5, 34, 34, 40, 70, 40, 41, 20,  2,  9,\n",
              "       58, 67, 32, 58, 20, 68,  3, 60, 62, 30, 62, 35, 37, 50, 68,  9, 28,\n",
              "       21, 44, 68, 45, 49, 55, 49, 32, 56, 66, 35, 59, 66, 14, 28, 14, 42,\n",
              "       45, 45, 24, 71, 60, 55, 44, 33, 18, 24, 73, 24, 34, 25, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTRV9u1EUL7W",
        "outputId": "805ca3ec-2a07-4968-a8b8-184bd71bb8f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b' it as well as she could.\\r\\n\\r\\n\\xe2\\x80\\x9cThe game\\xe2\\x80\\x99s going on rather better now,\\xe2\\x80\\x9d she said, by way of keeping up'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'KM\\nZ_yf)T\\xe2\\x80\\x99VzoHNL[UNK]g[UNK]rkkmK\\xe2\\x80\\x9ceYadfQ.B!Q\\rg\\ntEc(UU[\\xe2\\x80\\x94[]G\\r-pySpGz rtQtVXhz-OHbzcgmgSnxVqxAOA_ccK\\xe2\\x80\\x98rmbTEK\\xe2\\x80\\x9cKULL'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0].numpy()))\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "jrhwKR3rLYfJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0kux6nuTgvs"
      },
      "source": [
        "## Train the Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmxt0caociVW"
      },
      "source": [
        "### Attach and optimizer and a loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyJ17u_cYQL-",
        "outputId": "587076b7-d8c5-456b-dac8-90079a8b1987"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "target_example_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3lmUhq6YTfa",
        "outputId": "fcea52f4-15c0-4521-e46d-d93285675ba3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 100, 75])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "example_batch_predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "JX6V_Tn3YbRv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "bgWhxzL4Tidr"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPycZItRUBCl",
        "outputId": "a4f41362-a3f8-4c62-a88a-a283d3f91f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 75)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.317149\n"
          ]
        }
      ],
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lma6UryrpMLW",
        "outputId": "7dcb5700-208c-49b1-ad4d-d7183262cc7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74.97458"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "tf.exp(mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "RlnGxRY0cYGB"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "NG-eJ0VAcZsC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKwIpq_bcr3w"
      },
      "source": [
        "### Configure Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "pUnLuKUrXkjg"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "JMDKUzBjc8tF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW-_-7w-dJXU"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "JcQD5LttdMUC"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAZldkY4dNxh",
        "outputId": "dd54b407-c912-4b47-94c5-a941d5890fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "22/22 [==============================] - 6s 148ms/step - loss: 4.0848\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 3s 130ms/step - loss: 3.0662\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 4s 148ms/step - loss: 2.6695\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 4s 133ms/step - loss: 2.4148\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 3s 130ms/step - loss: 2.2958\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 3s 132ms/step - loss: 2.2129\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 3s 132ms/step - loss: 2.1366\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 4s 145ms/step - loss: 2.0679\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 4s 143ms/step - loss: 1.9954\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 4s 142ms/step - loss: 1.9206\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 4s 140ms/step - loss: 1.8482\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 4s 140ms/step - loss: 1.7781\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 3s 135ms/step - loss: 1.7143\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 3s 132ms/step - loss: 1.6522\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 3s 135ms/step - loss: 1.5930\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 1.5399\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 3s 131ms/step - loss: 1.4820\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 3s 130ms/step - loss: 1.4284\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 1.3792\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 1.3266\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 1.2779\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 1.2305\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 3s 132ms/step - loss: 1.1798\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 3s 137ms/step - loss: 1.1370\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 3s 137ms/step - loss: 1.0912\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 3s 136ms/step - loss: 1.0432\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 3s 135ms/step - loss: 0.9962\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.9489\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 3s 135ms/step - loss: 0.9024\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 3s 132ms/step - loss: 0.8513\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.7983\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 0.7431\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 3s 132ms/step - loss: 0.6908\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 3s 135ms/step - loss: 0.6366\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 3s 132ms/step - loss: 0.5839\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.5289\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.4748\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 3s 140ms/step - loss: 0.4261\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 3s 138ms/step - loss: 0.3770\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 3s 139ms/step - loss: 0.3373\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.2989\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 3s 131ms/step - loss: 0.2633\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 3s 131ms/step - loss: 0.2344\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 3s 131ms/step - loss: 0.2076\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.1852\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 3s 139ms/step - loss: 0.1642\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 3s 132ms/step - loss: 0.1465\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 3s 135ms/step - loss: 0.1321\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 3s 132ms/step - loss: 0.1217\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 3s 132ms/step - loss: 0.1122\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "rKlBcrQGdPie"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9AMMOd5eE_N"
      },
      "source": [
        "### Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "JxGiVRVxeJQw"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7LMBTuYaeNiU"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwVK2niVePQK",
        "outputId": "64a9e45b-bb78-469b-cbf4-ca2ccd51d55c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAPTER: Afiting its even give in having head to find my weak\r\n",
            "in the world! Oh, my dook, I’ve sorntemed to make it scrowl of the sly.\r\n",
            "\r\n",
            "“In that case, escope!” said Alice; “as you pleased a\r\n",
            "moment to be Number of comifir.”\r\n",
            "\r\n",
            "“It must be a very pretty jame, the Duchess only getting on the sont,” she said to herself, for she had\r\n",
            "drunk half the bottle, she found her head pressing against the door, and the other was sitting on the thing,\r\n",
            "but it is all the things get is: but she knew the White Rabbit was still in sight, hurriedly went on.\r\n",
            "\r\n",
            "“I do,” Alice has veid nothing but out-of-the-way\r\n",
            "things to happen, that it seemed quite nabused impatiently until it caws befween into a comfort, one way—never to be a book of\r\n",
            "little cartwheels, and the sound of a good many voices all\r\n",
            "dogithout trying, that she let the Lozying of the hall in her lef on the game\r\n",
            "neck again, statid “The fartages and question, Ubblictly as she chose, for some wine of gleas togething about it is all she cradded, duze awa \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.27181339263916\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['CHAPTER:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "9pCnLM1heRd5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcNBj_MO7ZT5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-PO4yik7lU8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Generating_Text_Using_RNNS.ipynb",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPzuNiQS9MJy91kS81/erR8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}